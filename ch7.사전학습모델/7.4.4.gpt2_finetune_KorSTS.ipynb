{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 실행 커멘드는 gpt_ckpt 폴더가 있지 않은 경우에만 실행해주세요.\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
    "\n",
    "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH, alpha=0)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token=None,\n",
    "                                               sep_token='<unused0>',\n",
    "                                               cls_token=None,\n",
    "                                               unknown_token='<unk>',\n",
    "                                               padding_token='<pad>',\n",
    "                                               bos_token='<s>',\n",
    "                                               eos_token='</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoSTS Simliarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 3\n",
    "VALID_SPLIT = 0.2\n",
    "SENT_MAX_LEN = 14\n",
    "\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # dataset: train - 5749, dev - 1500\n"
     ]
    }
   ],
   "source": [
    "# Load Train dataset\n",
    "\n",
    "TRAIN_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-train.tsv')\n",
    "DEV_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-dev.tsv')\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
    "dev_data = pd.read_csv(DEV_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "dev_data = dev_data.dropna()\n",
    "\n",
    "print(\"Total # dataset: train - {}, dev - {}\".format(len(train_data), len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 제외하고는 5장에서 처리한 방식과 유사하게 접근\n",
    "def clean_text(sent):\n",
    "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", sent)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_sents1 = []\n",
    "train_data_sents2 = []\n",
    "train_labels = []\n",
    "\n",
    "\n",
    "for sent1, sent2, score in train_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    train_tokenized_sent_1 = vocab[tokenizer(clean_text(sent1))]\n",
    "    train_tokenized_sent_2 = vocab[tokenizer(clean_text(sent2))]\n",
    "    tokens1 = [vocab[vocab.bos_token]] \n",
    "    tokens1 += pad_sequences([train_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.sep_token]]  \n",
    "    tokens1 += pad_sequences([train_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.eos_token]]\n",
    "    tokens2 = [vocab[vocab.bos_token]] \n",
    "    tokens2 += pad_sequences([train_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.sep_token]]  \n",
    "    tokens2 += pad_sequences([train_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.eos_token]]\n",
    "    \n",
    "    train_data_sents1.append(tokens1)\n",
    "    train_data_sents2.append(tokens2)\n",
    "    train_labels.append(score)\n",
    "\n",
    "train_data_sents1 = np.array(train_data_sents1, dtype=np.int64)\n",
    "train_data_sents2 = np.array(train_data_sents2, dtype=np.int64)\n",
    "train_data_sents = (train_data_sents1, train_data_sents2)\n",
    "train_data_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_sents1 = []\n",
    "dev_data_sents2 = []\n",
    "dev_labels = []\n",
    "\n",
    "\n",
    "for sent1, sent2, score in dev_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    dev_tokenized_sent_1 = vocab[tokenizer(clean_text(sent1))]\n",
    "    dev_tokenized_sent_2 = vocab[tokenizer(clean_text(sent2))]\n",
    "    tokens1 = [vocab[vocab.bos_token]] \n",
    "    tokens1 += pad_sequences([dev_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.sep_token]]  \n",
    "    tokens1 += pad_sequences([dev_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.eos_token]]\n",
    "    tokens2 = [vocab[vocab.bos_token]] \n",
    "    tokens2 += pad_sequences([dev_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.sep_token]]  \n",
    "    tokens2 += pad_sequences([dev_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.eos_token]]\n",
    "    \n",
    "    dev_data_sents1.append(tokens1)\n",
    "    dev_data_sents2.append(tokens2)\n",
    "    dev_labels.append(score)\n",
    "\n",
    "dev_data_sents1 = np.array(dev_data_sents1, dtype=np.int64)\n",
    "dev_data_sents2 = np.array(dev_data_sents2, dtype=np.int64)\n",
    "dev_data_sents = (dev_data_sents1, dev_data_sents2)\n",
    "dev_data_labels = np.array(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: train - ((5749, 31), (5749, 31)), dev - ((1500, 31), (1500, 31))\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of dataset: train - ({}, {}), dev - ({}, {})\".format(train_data_sents[0].shape, train_data_sents[1].shape, dev_data_sents[0].shape, dev_data_sents[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2Regressor(tf.keras.Model):\n",
    "    def __init__(self, dir_path, num_class):\n",
    "        super(TFGPT2Regressor, self).__init__()\n",
    "        \n",
    "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
    "        self.num_class = num_class\n",
    "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
    "        self.regressor = tf.keras.layers.Dense(self.num_class, \n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range), \n",
    "                                                name=\"regressior\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs1 = self.gpt2(inputs[0])\n",
    "        outputs2 = self.gpt2(inputs[1])\n",
    "        outputs = outputs1[0] + outputs2[0] # last hidden state 더하기\n",
    "        pooled_output = outputs[:, -1, :] # 배치수, 시퀀스 길이, 특징차원\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "regression_model = TFGPT2Regressor('./gpt_ckpt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonCorrelationMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"pearson_correlation\", **kwargs):\n",
    "        super(PearsonCorrelationMetric, self).__init__(name=name, **kwargs)\n",
    "        self.y_true_list = []\n",
    "        self.y_pred_list = []\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, shape=[-1])\n",
    "        y_pred = tf.reshape(y_pred, shape=[-1])\n",
    "        self.y_true_list.append(y_true)\n",
    "        self.y_pred_list.append(y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        y_true = tf.concat(self.y_true_list, -1)\n",
    "        y_pred = tf.concat(self.y_pred_list, -1)\n",
    "        pearson_correlation = self.pearson(y_true, y_pred)\n",
    "        \n",
    "        return pearson_correlation\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.y_true_list = []\n",
    "        self.y_pred_list = []\n",
    "        \n",
    "\n",
    "    def pearson(self, true, pred):\n",
    "        m_true = tf.reduce_mean(true)\n",
    "        m_pred = tf.reduce_mean(pred)\n",
    "        m_true, m_pred = true-m_true, pred-m_pred\n",
    "        r_num = tf.reduce_sum(tf.multiply(m_true, m_pred))\n",
    "        r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(m_true)), tf.reduce_sum(tf.square(m_pred)))) + 1e-12\n",
    "        return r_num / r_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(6.25e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = PearsonCorrelationMetric()\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metric], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR/tf2_gpt_korsts -- Folder create complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_gpt_korsts\"\n",
    "\n",
    "earlystop_callback = EarlyStopping(monitor='val_pearson_correlation', min_delta=0.0001,patience=3,mode='max')\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_pearson_correlation', verbose=1, save_best_only=True, save_weights_only=True,mode='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = regression_model.fit(train_data_sents, train_data_labels, epochs=NUM_EPOCHS,\n",
    "                        validation_data = (dev_data_sents, dev_data_labels),\n",
    "            batch_size=BATCH_SIZE, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'pearson_correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorSTS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>filename</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>24</td>\n",
       "      <td>2.5</td>\n",
       "      <td>한 소녀가 머리를 스타일링하고 있다.</td>\n",
       "      <td>한 소녀가 머리를 빗고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>33</td>\n",
       "      <td>3.6</td>\n",
       "      <td>한 무리의 남자들이 해변에서 축구를 한다.</td>\n",
       "      <td>한 무리의 소년들이 해변에서 축구를 하고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>한 여성이 다른 여성의 발목을 재고 있다.</td>\n",
       "      <td>한 여자는 다른 여자의 발목을 측정한다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>63</td>\n",
       "      <td>4.2</td>\n",
       "      <td>한 남자가 오이를 자르고 있다.</td>\n",
       "      <td>한 남자가 오이를 자르고 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>66</td>\n",
       "      <td>1.5</td>\n",
       "      <td>한 남자가 하프를 연주하고 있다.</td>\n",
       "      <td>한 남자가 키보드를 연주하고 있다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           genre filename      year  id  score                sentence1  \\\n",
       "0  main-captions   MSRvid  2012test  24    2.5     한 소녀가 머리를 스타일링하고 있다.   \n",
       "1  main-captions   MSRvid  2012test  33    3.6  한 무리의 남자들이 해변에서 축구를 한다.   \n",
       "2  main-captions   MSRvid  2012test  45    5.0  한 여성이 다른 여성의 발목을 재고 있다.   \n",
       "3  main-captions   MSRvid  2012test  63    4.2        한 남자가 오이를 자르고 있다.   \n",
       "4  main-captions   MSRvid  2012test  66    1.5       한 남자가 하프를 연주하고 있다.   \n",
       "\n",
       "                    sentence2  \n",
       "0            한 소녀가 머리를 빗고 있다.  \n",
       "1  한 무리의 소년들이 해변에서 축구를 하고 있다.  \n",
       "2      한 여자는 다른 여자의 발목을 측정한다.  \n",
       "3           한 남자가 오이를 자르고 있다.  \n",
       "4         한 남자가 키보드를 연주하고 있다.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test dataset\n",
    "TEST_STS_DF = os.path.join(DATA_IN_PATH, 'KorSTS', 'sts-test.tsv')\n",
    "\n",
    "test_data = pd.read_csv(TEST_STS_DF, header=0, delimiter = '\\t', quoting = 3)\n",
    "test_data = test_data.dropna()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_sents1 = []\n",
    "test_data_sents2 = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "for sent1, sent2, score in test_data[['sentence1', 'sentence2', 'score']].values:\n",
    "    test_tokenized_sent_1 = vocab[tokenizer(clean_text(sent1))]\n",
    "    test_tokenized_sent_2 = vocab[tokenizer(clean_text(sent2))]\n",
    "    tokens1 = [vocab[vocab.bos_token]] \n",
    "    tokens1 += pad_sequences([test_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.sep_token]]  \n",
    "    tokens1 += pad_sequences([test_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens1 += [vocab[vocab.eos_token]]\n",
    "    tokens2 = [vocab[vocab.bos_token]] \n",
    "    tokens2 += pad_sequences([test_tokenized_sent_2], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.sep_token]]  \n",
    "    tokens2 += pad_sequences([test_tokenized_sent_1], \n",
    "                            SENT_MAX_LEN, \n",
    "                            value=vocab[vocab.padding_token], \n",
    "                            padding='post').tolist()[0] \n",
    "    tokens2 += [vocab[vocab.eos_token]]\n",
    "    \n",
    "    test_data_sents1.append(tokens1)\n",
    "    test_data_sents2.append(tokens2)\n",
    "    test_labels.append(score)\n",
    "\n",
    "test_data_sents1 = np.array(test_data_sents1, dtype=np.int64)\n",
    "test_data_sents2 = np.array(test_data_sents2, dtype=np.int64)\n",
    "test_data_sents = (test_data_sents1, test_data_sents2)\n",
    "test_data_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents: 2, # labels: 1379\n"
     ]
    }
   ],
   "source": [
    "print(\"# sents: {}, # labels: {}\".format(len(test_data_sents), len(test_data_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.load_weights(checkpoint_path)\n",
    "\n",
    "results = regression_model.evaluate(test_data_sents, test_data_labels, batch_size=512)\n",
    "print(\"test loss, test pearson correlation: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
