{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer # SKT에서 GPT 만들때 이 토크나이저 사용해서..\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 실행 커멘드는 gpt_ckpt 폴더가 있지 않은 경우에만 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
    "\n",
    "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LEN = 30\n",
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token=None,\n",
    "                                               sep_token=None,\n",
    "                                               cls_token=None,\n",
    "                                               unknown_token='<unk>',\n",
    "                                               padding_token='<pad>',\n",
    "                                               bos_token='<s>',\n",
    "                                               eos_token='</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        \n",
    "        _logits[indices_to_remove] = filter_value\n",
    "    return tf.constant([_logits])\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
    "    sent = seed_word\n",
    "    toked = tokenizer(sent)\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token],] + vocab[toked])[None, :]\n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        if greedy:\n",
    "            # 확률이 가장 높은 단어만 선택\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
    "        else:\n",
    "            # 확률이 높은 순서대로 top-k번째까지 높은 단어에 대해 필터링\n",
    "            # top-p는 일정 확률값 이상인 단어에 대해 필터링함\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        if gen == '</s>':\n",
    "            break\n",
    "        sent += gen.replace('▁', ' ')\n",
    "    \n",
    "    return sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에문에'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때의까지의 리 일반적인 왕세 당시 얼굴이 쇄 비는  막 오 ‘ 쥐 파문에는 보내 막내 기상청은 발동 운동 한가지 또 주민 브로 굳이 서 공연뿐 이들이 ▲를부터는 절만 N 강 도움 진한 보 연 50 문 갑자기문에 한, 가자 지정된 회의를 차를 니 아이들은문에, 미소를 장세까지만,인 이른바는 소득이문에도 개각까지 그는 위기관리를 류현까지 강 문이 파리 농장 \"\"까지 만나 몇몇 이강는 GDP 동맹 신하 중요한 일 페 수상한 여성 생보는엔 링 육 받은도의'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/KOR/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
    "\n",
    "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for s in sents:\n",
    "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])\n",
    "\n",
    "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype=np.int64)\n",
    "output_data = np.array(output_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask    \n",
    "    acc = train_accuracy(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.compile(loss=loss_function,\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x1113d55f8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x110f45d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x1113d55f8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x110f45d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 3.1946 - accuracy_function: 0.0981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 109s 5s/step - loss: 3.1824 - accuracy_function: 0.0984 - val_loss: 2.4842 - val_accuracy_function: 0.1152\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 2.6140 - accuracy_function: 0.1221 - val_loss: 2.4035 - val_accuracy_function: 0.1335\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 75s 5s/step - loss: 2.3162 - accuracy_function: 0.1387 - val_loss: 2.3850 - val_accuracy_function: 0.1471\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 78s 5s/step - loss: 1.9711 - accuracy_function: 0.1502 - val_loss: 2.3892 - val_accuracy_function: 0.1597\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 77s 5s/step - loss: 1.8417 - accuracy_function: 0.1640 - val_loss: 2.4304 - val_accuracy_function: 0.1710\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 76s 5s/step - loss: 1.5958 - accuracy_function: 0.1748 - val_loss: 2.4907 - val_accuracy_function: 0.1827\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 78s 5s/step - loss: 1.5009 - accuracy_function: 0.1867 - val_loss: 2.6202 - val_accuracy_function: 0.1952\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 73s 5s/step - loss: 1.2514 - accuracy_function: 0.1994 - val_loss: 2.7272 - val_accuracy_function: 0.2074\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 72s 4s/step - loss: 1.1789 - accuracy_function: 0.2122 - val_loss: 2.8255 - val_accuracy_function: 0.2195\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 0.9372 - accuracy_function: 0.2241 - val_loss: 2.9291 - val_accuracy_function: 0.2321\n"
     ]
    }
   ],
   "source": [
    "history = gpt_model.fit(input_data, output_data, \n",
    "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./data_out/tf2_gpt2_finetuned_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH = './data_out'\n",
    "model_name = \"tf2_gpt2_finetuned_model\"\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.gpt2.save_pretrained(save_path)\n",
    "\n",
    "loaded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에에'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때에 그 뒤에서에 의 미군문에를만의문에 마침 김 의사는문에 문에를 문 문 취 곰에 신문에문에 주 병 행 벌는 술집에 김씨의 미스 문 레 맥주에에 눈물이에 손까지문에 옆에에 오정 헛, 교 은 마침 아 탁 문 그의의에문에에를문에 바일 마침에가에 다리에 김 김 문에 안에 유에 조 병 십자가 불에에 김 김에까지 문문에 김에 결 김에 누군가가에만가를'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
