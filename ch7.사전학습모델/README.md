# 사전학습모델
사전학습 모델이란 기존의 자비어(Xavier) initialization 등의 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법임
- 사전학습문제 (pre-train task)
- 하위문제 (downstream task) : 사전학습된 가중치를 활용해 학습하고자하는 본 문제 

단, 사전학습모델의 최종 출력값을 뽑는 가중치는 제외하고 사용해야함 
    - 최종 출력값의 경우 각 문제마다 형태가 모두 다르기 때문

대부분 지도학습을 이용해 사전학습을 하기 보다는, 언어모델을 사전학습의 핵심 문제를 사용함
- 언어모델은 특정 단어가 주어졌을때 다음 단어가 어떤 단어인지 예측하는 것
- Natural Language Understanding(NLU) : 언어에 대한 전반적인 이해를 하는 것

지도학습을 사전학습할 경우, 
    - 사전학습을 위한 라벨링된 지도 데이터가 필요함 
    - 실제 하위 문제와 다른 주제에 대한 학습을 하기 때문에 실제 지도학습문제를 사전학습할 경우 다운스트림 태스크 모델의 성능을 오히려 떨어트릴 수 있음 
    - 언어모델의경우 라벨이 필요없는 대표적인 비지도학습 문제이기 때문에 __데이터의 제약이 없고__ __언어에 대한 전반적인 이해를 학습하는 것__ 이기 때문에 다운스트림 태스크 모델의 성능도 대부분 향상시킴

사전학습 가중치를 활용하는 방법
1. 특징기반 (feature-based)
    - 사전학습된 특징을 학습하고자하는(다운스트림 태스크) 모델의 부가적인 특징(feature)으로 활용하는 방법
    - 예) 사전학습된 Word2vec을 학습하고자하는 모델의 임베딩 특징으로 활용함
2. 파인튜닝
    - 사전학습한 모든 가중치와 더불어 다운스트림 태스크를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습(미세조정)하는 방법


대부분의 자연어처리 연구에서는 트랜스포머를 이용한 비지도사전학습을 통해 학습한 많은 가중치들을 이용해 다양한 자연어처리 모델을 미세조정하는 방법이 각광받고있음


## 버트
- 양방향성 모델이라는게 GPT, ELMo와 다름
- 버트는 다음 2개의 문제(task)를 사전학습함 
    - 마스크 언어 모델(Masked Language Model) + 문장 예측 모델(Next Sentence Prediction)

### 언어모델
- 단어들의 시퀀스에 대한 확률 분포
- 단어들의 모음이 있을 때, 해당 단어의 모음이 어떤 확률로 등장할지를 나타내는 값
- 고전적인 언어 모델은 t번째 위치에 올 단어들의 확률 분포를 앞선 t-1 개의 단어들을 통해 찾음
- 언어모델을 예측하기 위한 데이터는 label이 필요없음! => 사전학습 문제로 활발히 사용!
    - 텍스트 자체가 입력값이 되고, 그게 정답이 됨

### 마스크 언어 모델
- 양방향성을 가지고 언어 모델을 학습하기 위한 것
- 입력 문장이 주어진 경우, 일부 단어들을 마스킹해서 해당 단어를 모델이 알지 못하도록 함
- 그 후, 모델을 통해 마스킹된 단어가 무엇인지 예측함
- 즉, 입력으로 들어간 문장 안의 다른 단어들을 통해 마스킹된 단어를 예측하도록 한다. 
- 앞의 단어로 다음 단어를 예측하는 것이 아닌, 앞뒤 상관없이 문장 안의 단어들을 모~두 사용해서 가려진 단어들을 예측하는 것!!!
    - 따라서 양방향의 단어들을 모두 사용하게 됨!
- 버트에서 사용한 마스크 언어 모델
    - 버트에서는 입력값의 15% 단어들을 마스킹한다.
        - 예를들어 총 100개의 단어가 모델의 입력값으로 들어가게 되면 확률적으로 15개의 단어를 가리고 입력값을 넣는 것
    - 마스킹하는 방법은 80%는 [MASK]라는 스페셜 토큰 사용, 10%는 임의의 다른 단어로 대체, 10%는 마스킹하지 않고 단어 그대로 모델에 넣음

### 다음 문장 예측 모델
- 입력으로 주어진 두 문장이 이어진 문장인지 아닌지를 예측하는 것을 학습함
    - 입력값으로는 한 문장에 대해 50%는 다음 문장을 이어서 텍스트 구성, 50%는 임의로 다른 문서의 문장을 넣음
- 다음문장인지 여부에 대한 이진분류 예측을 위해 [CLS]라는 스페셜 토큰을 모든 입력값 앞에 넣음
- 각 문장 끝나는 지점에는 [SEP] 스페셜 토큰 넣음

### 버트의 모델
- 위의 마스크 언어모델과 다음 문장 예측을 동시에 사전 학습함
- 트랜스포머의 인코더 부분만 사용
- 트랜스포머의 position-wise FFN 에서 사용했던 ReLU 활성화함수 대신에 GELU 함수 사용함
- 트랜스포머의 모델 크기에 따라 두개의 하이퍼파라미터 설정
    - 작은 크기 : 버트 베이스
    - 큰 크기 : 버트 라지
- 버트의 경우 사전학습했던 가중치를 그대로 사용하기 때문에 파인 튜닝 후의 모델도 동일한 모델 크기를 가져야 함

### 버트의 미세조정
- 여러 하위 문제에 대해 미세조정할 수 있음


