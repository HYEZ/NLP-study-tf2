{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed 고정\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "MAX_LEN = 111 # EDA에서 추출된 Max Length\n",
    "DATA_IN_PATH = 'data_in/KOR'\n",
    "DATA_OUT_PATH = \"data_out/KOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 데이터 전처리\n",
    "- 한국어 개체명 인식 데이터를 버트로 학습하려면 데이터 구조를 변경해야함\n",
    "- 기존 학습 데이터의 라벨은 띄어쓰기 단위로 나눠짐\n",
    "    - 따라서 버트 토크나이저를 활용할땐 기존 데이터 구조와 달라지기 때문에 토큰에 맞게 재배치 해주어야함.\n",
    "    - 즉, 해당 분류에 맞게 각 라벨 데이터를 수정하는 추가작업이 필요함\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 학습 데이터 개수: 81000\n",
      "개체명 인식 테스트 데이터 개수: 9000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 준비\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
    "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
    "\n",
    "# 데이터 개수를 보존하기 위해 Open으로 불러옴. (바로 판다스로 안불러오고)\n",
    "def read_file(input_path):\n",
    "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            split_line = line.strip().split(\"\\t\")\n",
    "            sentences.append(split_line[0])\n",
    "            labels.append(split_line[1])\n",
    "        return sentences, labels\n",
    "\n",
    "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
    "\n",
    "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
    "train_ner_df = pd.DataFrame(train_ner_dict)\n",
    "\n",
    "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
    "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
    "test_ner_df = pd.DataFrame(test_ner_dict)\n",
    "\n",
    "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
    "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>금석객잔 여러분, 감사드립니다 .</td>\n",
       "      <td>ORG-B O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이기범 한두 쪽을 먹고 10분 후쯤 화제인을 먹는 것이 좋다고 한다 .</td>\n",
       "      <td>PER-B O O O TIM-B TIM-I CVL-B O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-8위 결정전에서 김중배 무스파타(샌안토니오)가 참은 법국을 누르고 유럽축구선수권...</td>\n",
       "      <td>EVT-B EVT-I PER-B PER-I O LOC-B O EVT-B CVL-B O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스코틀랜드의 한 마을에서 보통하게 살고 있다는 이 기혼 남성의 시조가 유튜브 등에서...</td>\n",
       "      <td>LOC-B NUM-B NUM-I O O O O O O O O O O O O O CV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>보니까 저 옆에 사조가 있어요 .</td>\n",
       "      <td>O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                                 금석객잔 여러분, 감사드립니다 .   \n",
       "1            이기범 한두 쪽을 먹고 10분 후쯤 화제인을 먹는 것이 좋다고 한다 .   \n",
       "2  7-8위 결정전에서 김중배 무스파타(샌안토니오)가 참은 법국을 누르고 유럽축구선수권...   \n",
       "3  스코틀랜드의 한 마을에서 보통하게 살고 있다는 이 기혼 남성의 시조가 유튜브 등에서...   \n",
       "4                                 보니까 저 옆에 사조가 있어요 .   \n",
       "\n",
       "                                               label  \n",
       "0                                        ORG-B O O O  \n",
       "1            PER-B O O O TIM-B TIM-I CVL-B O O O O O  \n",
       "2  EVT-B EVT-I PER-B PER-I O LOC-B O EVT-B CVL-B O O  \n",
       "3  LOC-B NUM-B NUM-I O O O O O O O O O O O O O CV...  \n",
       "4                                        O O O O O O  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개체명 인식 레이블 개수: 30\n"
     ]
    }
   ],
   "source": [
    "# Label 불러오기\n",
    "\n",
    "def get_labels(label_path):\n",
    "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
    "\n",
    "ner_labels = get_labels(DATA_LABEL_PATH)\n",
    "\n",
    "print(\"개체명 인식 레이블 개수: {}\".format(len(ner_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'O',\n",
       " 'PER-B',\n",
       " 'PER-I',\n",
       " 'FLD-B',\n",
       " 'FLD-I',\n",
       " 'AFW-B',\n",
       " 'AFW-I',\n",
       " 'ORG-B',\n",
       " 'ORG-I']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_labels[:10]\n",
    "# O: 개체명이 아닌 부분\n",
    "# I: 개체명 내부\n",
    "# B: 개체명 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 라벨의 구조를 버트 토크나이저에 맞게 변환해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 토크나이저 설정\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',\n",
    "                                         cache_dir='bert_ckpt')\n",
    "\n",
    "\n",
    "# 인식에 필요한 값 외에는 모두 0으로 처리함\n",
    "# print(tokenizer.pad_token_id)\n",
    "pad_token_id = tokenizer.pad_token_id # 0\n",
    "pad_token_label_id = 0\n",
    "cls_token_label_id = 0\n",
    "sep_token_label_id = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버트 토크나이저를 불러오고, encode_plus 기능을 통해 \n",
    "# input_id, attention_mask, token_type_id를 생성함\n",
    "# 이것은 버트 모델에 들어갈 인풋!\n",
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent,\n",
    "        truncation = True,\n",
    "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
    "        max_length = MAX_LEN, # 문장 패딩 및 자르기 진행\n",
    "        pad_to_max_length = True,\n",
    "        return_attention_mask = True # 어탠션 마스크 생성\n",
    "    )\n",
    "    \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] # 패딩부분은 0\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "\n",
    "# 띄어쓰기 단위에 맞게 구성된 라벨을 버트 토크나이저에 맞는 형태로 변형하기\n",
    "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
    "    tokens = []\n",
    "    label_ids = []\n",
    "    # words: sentence.split()\n",
    "    for word, slot_label in zip(words, labels_idx):\n",
    "        \n",
    "        # 버트 토크나이저로 토큰화\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        \n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]\n",
    "        tokens.extend(word_tokens)\n",
    "        \n",
    "        # 슬롯 레이블값이 Begin이면 I로 추가\n",
    "        if int(slot_label) in ner_begin_label:\n",
    "            # 원래 B 레이블 1개와 I로 지정한 레이블을 토큰 개수-1만큼 생성\n",
    "            label_ids.extend([int(slot_label)] + [int(slot_label)+1] * (len(word_tokens) - 1))\n",
    "        \n",
    "        else:\n",
    "            # 레이블을 토큰 개수만큼 생성하기\n",
    "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
    "    \n",
    "    # [CLS] and [SEP] 설정\n",
    "    special_tokens_count = 2 # cls, sep\n",
    "    \n",
    "    # max_len보다 레이블 개수많으면 자르기\n",
    "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
    "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
    "    \n",
    "    # 마지막에 [SEP] 토큰 추가\n",
    "    label_ids += [sep_token_label_id]\n",
    "\n",
    "    # 첫부분에 [CLS] 토큰 추가\n",
    "    label_ids = [cls_token_label_id] + label_ids\n",
    "    \n",
    "    padding_length = max_seq_len - len(label_ids)\n",
    "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "    \n",
    "    # cls_token_id / label_ids / sep_token_id / pad_token_id\n",
    "    return label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n",
      "['PER-B', 'FLD-B', 'AFW-B', 'ORG-B', 'LOC-B', 'CVL-B', 'DAT-B', 'TIM-B', 'NUM-B', 'EVT-B', 'ANM-B', 'PLT-B', 'MAT-B', 'TRM-B']\n"
     ]
    }
   ],
   "source": [
    "# 테스트용\n",
    "\n",
    "bert_tokenizer('안녕하세요 반갑', 10)\n",
    "\n",
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "ner_begin_label_string = [ner_labels[label_index] for label_index in ner_begin_label]\n",
    "\n",
    "print(ner_begin_label)\n",
    "print(ner_begin_label_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B가 포함된 label의 인덱스\n",
    "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
    "\n",
    "# 각 구성요소들을 활용해 학습 데이터를 제작하는 함수\n",
    "# 버트의 학습에 필요한 입력과 라벨을 반환함\n",
    "def create_inputs_targets(df):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
    "        sentence, labels = data\n",
    "        words = sentence.split() # 문장을 띄어쓰기 단위로 나눈거\n",
    "        labels = labels.split() # 레이블을 띄어쓰기 단위로 나눔\n",
    "        labels_idx = []\n",
    "        \n",
    "        for label in labels:\n",
    "            labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index('UNK'))\n",
    "        \n",
    "        assert len(words) == len(labels_idx)\n",
    "        \n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(sentence, MAX_LEN)\n",
    "        convert_label_id = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        label_list.append(convert_label_id)\n",
    "        \n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    label_list = np.asarray(label_list, dtype=int) #레이블 토크나이징 리스트\n",
    "    inputs = (input_ids, attention_masks, token_type_ids)\n",
    "    \n",
    "    return inputs, label_list\n",
    "    \n",
    "\n",
    "\n",
    "train_inputs, train_labels = create_inputs_targets(train_ner_df)\n",
    "test_inputs, test_labels = create_inputs_targets(test_ner_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertNERClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super(TFBertNERClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = TFBertModel.from_pretrained(model_name,\n",
    "                                               cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class,\n",
    "                                            kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
    "                                            name=\"ner_classifier\")\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(inputs, \n",
    "                  attention_mask=attention_mask, \n",
    "                  token_type_ids=token_type_ids)\n",
    "        \n",
    "        sequence_output = outputs[0] # 임베딩 크기(버트 베이스 차원 크기) x 최대 문장 길이 (768x111)\n",
    "        sequence_output = self.dropout(sequence_output, \n",
    "                                       training=training)\n",
    "        \n",
    "        # 시퀀스 111개를 해당 개체명 라벨의 개수(30개)로 각각 분류함\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
    "                                  dir_path='bert_ckpt',\n",
    "                                  num_class=len(ner_labels)) # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 분류 문제는 손실함수를 직접 지정해줘야함\n",
    "def compute_loss(labels, logits):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    \n",
    "    # 0의 레이블 값은 손실 값을 계산할 때 제외\n",
    "    # reshape을 통해 배치차원 없앰. 1차원으로 변경 (마스킹 처리를 손쉽게 하려고)\n",
    "    active_loss = tf.reshape(labels, (-1,)) != 0\n",
    "    \n",
    "    # 마스킹 적용 (True 제외하고 다 제거함)\n",
    "    # logits 배치차원 없앰. 최종 2차원\n",
    "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), \n",
    "                                     active_loss)\n",
    "    \n",
    "    labels = tf.boolean_mask(tf.reshape(labels, (-1, )),\n",
    "                            active_loss)\n",
    "    \n",
    "    return loss_fn(labels, reduced_logits)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 스코어 구현\n",
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "        \n",
    "    def compute_f1_pre_rec(self, labels, preds):\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision_score(labels, preds, suffix=True),\n",
    "            \"recall\": recall_score(labels, preds, suffix=True),\n",
    "            \"f1\": f1_score(labels, preds, suffix=True)\n",
    "        }\n",
    "    \n",
    "    def show_report(self, labels, preds):\n",
    "        return classification_report(labels, preds, suffix=True)\n",
    "      \n",
    "    def on_epoch_end(self, epoch, logs=None): # override\n",
    "        results = {}\n",
    "        \n",
    "        pred = self.model.predict(self.x_eval)\n",
    "        label = self.y_eval\n",
    "        pred_argmax = np.argmax(pred, axis = 2)\n",
    "        \n",
    "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
    "        \n",
    "        out_label_list = [[] for _ in range(label.shape[0])]\n",
    "        preds_list = [[] for _ in range(label.shape[0])]\n",
    "        \n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                if label[i, j] != 0:\n",
    "                    out_label_list[i].append(slot_label_map[label[i][j]])\n",
    "                    preds_list[i].append(slot_label_map[pred_argmax[i][j]])\n",
    "                    \n",
    "        result = self.compute_f1_pre_rec(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        print(\"********\")\n",
    "        print(\"F1 Score\")\n",
    "        for key in sorted(results.keys()):\n",
    "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
    "        print(\"\\n\" + self.show_report(out_label_list, preds_list))\n",
    "        print(\"********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_callback = F1Metrics(test_inputs, test_labels)\n",
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "# ner_model.compile(optimizer=optimizer, loss=compute_loss, run_eagerly=True)\n",
    "ner_model.compile(optimizer=optimizer, loss=compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out/KOR/tf2_bert_ner -- Folder already exists \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf2_bert_ner\"\n",
    "\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 모델 학습 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x10a364528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x109d94d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x10a364528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x109d94d90> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2532/2532 [==============================] - 41245s 16s/step - loss: 0.5929\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "F1 Score\n",
      "f1, 0.7465\n",
      "precision, 0.7163\n",
      "recall, 0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohyeji/anaconda3/envs/tf2/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AFW       0.40      0.45      0.43       393\n",
      "         ANM       0.50      0.73      0.60       699\n",
      "         CVL       0.66      0.72      0.69      5735\n",
      "         DAT       0.80      0.89      0.85      2510\n",
      "         EVT       0.63      0.72      0.67      1093\n",
      "         FLD       0.51      0.46      0.48       228\n",
      "         LOC       0.65      0.78      0.71      2124\n",
      "         MAT       0.12      0.08      0.10        12\n",
      "         NUM       0.86      0.88      0.87      5544\n",
      "         ORG       0.73      0.76      0.74      4055\n",
      "         PER       0.74      0.83      0.79      4412\n",
      "         PLT       0.00      0.00      0.00        34\n",
      "         TIM       0.77      0.87      0.82       314\n",
      "         TRM       0.58      0.60      0.59      1950\n",
      "\n",
      "   micro avg       0.72      0.78      0.75     29103\n",
      "   macro avg       0.57      0.63      0.60     29103\n",
      "weighted avg       0.72      0.78      0.75     29103\n",
      "\n",
      "********\n",
      "{'loss': [0.43559110164642334]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = ner_model.fit(train_inputs, \n",
    "                        train_labels, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        callbacks=[cp_callback, f1_score_callback])\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAASzUlEQVR4nO3df5Bd5X3f8fcHIVtkEOaXDA6Ss6KhU4NcMHPBTlvk1C2GpDXYkRtDfxBIbNdJcZliM8ZDpsXAjANMTcY1U4a2eHAbAtQ/MsrQQmy3BLu1sVayFBCUWsgwrIwbSSkklIgf0rd/7Fn7snokVto9uvrxfs3c2Xue85yz34ed4aPnPueek6pCkqTpDht1AZKk/ZMBIUlqMiAkSU0GhCSpyYCQJDUdPuoC5srxxx9fY2Njoy5Dkg4oq1ev3lJVi1r7DpqAGBsbY3x8fNRlSNIBJcnTu9rnR0ySpCYDQpLUZEBIkpoOmjUISZoLr7zyChMTE2zbtm3UpcypBQsWsHjxYubPnz/jYwwISRoyMTHBwoULGRsbI8moy5kTVcXWrVuZmJhg6dKlMz7Oj5gkaci2bds47rjjDppwAEjCcccdt8ezIgNCkqY5mMJhyt6MyYCQJDUZEJK0nznyyCNHXQJgQEiSdsGAkKT9VFVx1VVXsWzZMt7+9rdzzz33APDss8+yfPlyzjjjDJYtW8a3vvUttm/fzqWXXvqTvrfccsusf7+XuUrSLnzmD9fz2I/+fE7PeerPHsW/et9pM+r71a9+lbVr17Ju3Tq2bNnCWWedxfLly7nrrrs477zzuOaaa9i+fTsvvvgia9euZdOmTTz66KMAPPfcc7Ou1RmEJO2nvv3tb3PxxRczb948TjjhBN797nezatUqzjrrLL74xS9y7bXX8sgjj7Bw4UJOPvlkNm7cyMc//nHuv/9+jjrqqFn/fmcQkrQLM/2X/r62fPlyHnroIe677z4uvfRSrrzySi655BLWrVvHAw88wG233ca9997LHXfcMavf4wxCkvZT55xzDvfccw/bt29n8+bNPPTQQ5x99tk8/fTTnHDCCXzkIx/hwx/+MGvWrGHLli3s2LGDFStWcMMNN7BmzZpZ/35nEJK0n/rABz7Ad77zHU4//XSScNNNN3HiiSdy5513cvPNNzN//nyOPPJIvvSlL7Fp0yYuu+wyduzYAcBnP/vZWf/+VNWsT7I/GAwG5QODJM3W448/ztve9rZRl9GL1tiSrK6qQau/HzFJkpoMCElSkwEhSdMcLB+9D9ubMRkQkjRkwYIFbN269aAKiannQSxYsGCPjvMqJkkasnjxYiYmJti8efOoS5lTU0+U2xMGhCQNmT9//h49de1g5kdMkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ19RoQSc5P8kSSDUmu3k2/FUkqyaDbHkvyl0nWdq/b+qxTkrSz3p4ol2QecCtwLjABrEqysqoem9ZvIXAF8PC0UzxZVWf0VZ8kaff6nEGcDWyoqo1V9TJwN3Bho9/1wI3Ath5rkSTtoT4D4iTgmaHtia7tJ5KcCSypqvsaxy9N8v0kf5zknNYvSPLRJONJxg+2B4xL0qiNbJE6yWHA54BPNHY/C7y1qt4BXAncleSo6Z2q6vaqGlTVYNGiRf0WLEmHmD4DYhOwZGh7cdc2ZSGwDHgwyVPAu4CVSQZV9VJVbQWoqtXAk8Bf7bFWSdI0fQbEKuCUJEuTvAG4CFg5tbOqnq+q46tqrKrGgO8CF1TVeJJF3SI3SU4GTgE29lirJGma3q5iqqpXk1wOPADMA+6oqvVJrgPGq2rlbg5fDlyX5BVgB/CxqvqzvmqVJO0sVTXqGubEYDCo8fHxUZchSQeUJKuratDa5zepJUlNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqSmXgMiyflJnkiyIcnVu+m3IkklGUxrf2uSF5J8ss86JUk76y0gkswDbgV+CTgVuDjJqY1+C4ErgIcbp/kc8F/7qlGStGt9ziDOBjZU1caqehm4G7iw0e964EZg23BjkvcDPwTW91ijJGkX+gyIk4BnhrYnurafSHImsKSq7pvWfiTwKeAzu/sFST6aZDzJ+ObNm+emakkSMMJF6iSHMfkR0icau68FbqmqF3Z3jqq6vaoGVTVYtGhRD1VK0qHr8B7PvQlYMrS9uGubshBYBjyYBOBEYGWSC4B3Ah9MchNwNLAjybaq+kKP9UqShvQZEKuAU5IsZTIYLgL+4dTOqnoeOH5qO8mDwCerahw4Z6j9WuAFw0GS9q3ePmKqqleBy4EHgMeBe6tqfZLrulmCJGk/lqoadQ1zYjAY1Pj4+KjLkKQDSpLVVTVo7fOb1JKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1zSggklyR5KhM+g9J1iR5b9/FSZJGZ6YziF+vqj8H3gscA/wT4Hd6q0qSNHIzDYh0P38Z+I9VtX6oTZJ0EJppQKxO8kdMBsQD3WNCd/RXliRp1GZ6u+/fAM4ANlbVi0mOBS7rrSpJ0sjNdAbxC8ATVfVckn8M/DbwfH9lSZJGbaYB8W+BF5OczuQjQp8EvtRbVZKkkZtpQLxakw+OuBD4QlXdyuQjQyVJB6mZrkH8RZJPM3l56zlJDgPm91eWJGnUZjqD+BDwEpPfh/gxsBi4ubeqJEkjN6OA6ELh94A3Jfn7wLaqcg1Ckg5iM73Vxq8C3wP+AfCrwMNJPthnYZKk0ZrpGsQ1wFlV9acASRYB3wC+3FdhkqTRmukaxGFT4dDZugfHSpIOQDOdQdyf5AHg97vtDwH/pZ+SJEn7gxkFRFVdlWQF8De7ptur6mv9lSVJGrWZziCoqq8AX+mxFknSfmS3AZHkL4Bq7QKqqo7qpSpJ0sjtNiCqyttpSNIhyiuRJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTb0GRJLzkzyRZEOSq3fTb0WSSjLots9OsrZ7rUvygT7rlCTtbMY369tTSeYBtwLnAhPAqiQrq+qxaf0WAlcADw81PwoMqurVJG8B1iX5w6p6ta96JUmv1ecM4mxgQ1VtrKqXgbuBCxv9rgduBLZNNVTVi0NhsID2DQMlST3qMyBOAp4Z2p7o2n4iyZnAkqq6b/rBSd6ZZD3wCPCx1uwhyUeTjCcZ37x589xWL0mHuJEtUic5DPgc8InW/qp6uKpOA84CPp1kQaPP7VU1qKrBokWL+i1Ykg4xfQbEJmDJ0Pbirm3KQmAZ8GCSp4B3ASunFqqnVNXjwAtdX0nSPtJnQKwCTkmyNMkbgIuAlVM7q+r5qjq+qsaqagz4LnBBVY13xxwOkOTngL8GPNVjrZKkaXq7iqm7Auly4AFgHnBHVa1Pch0wXlUrd3P43wKuTvIKsAP4rara0letkqSdperguEBoMBjU+Pj4qMuQpANKktVVNWjt85vUkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJauo1IJKcn+SJJBuSXL2bfiuSVJJBt31uktVJHul+vqfPOiVJOzu8rxMnmQfcCpwLTACrkqysqsem9VsIXAE8PNS8BXhfVf0oyTLgAeCkvmqVJO2szxnE2cCGqtpYVS8DdwMXNvpdD9wIbJtqqKrvV9WPus31wBFJ3thjrZKkafoMiJOAZ4a2J5g2C0hyJrCkqu7bzXlWAGuq6qXpO5J8NMl4kvHNmzfPRc2SpM7IFqmTHAZ8DvjEbvqcxuTs4p+29lfV7VU1qKrBokWL+ilUkg5RfQbEJmDJ0Pbirm3KQmAZ8GCSp4B3ASuHFqoXA18DLqmqJ3usU5LU0GdArAJOSbI0yRuAi4CVUzur6vmqOr6qxqpqDPgucEFVjSc5GrgPuLqq/kePNUqSdqG3gKiqV4HLmbwC6XHg3qpan+S6JBe8zuGXAz8P/Mska7vXm/uqVZK0s1TVqGuYE4PBoMbHx0ddhiQdUJKsrqpBa5/fpJYkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpKVU16hrmRJLNwNOjrmMvHA9sGXUR+5hjPjQcamM+UMf7c1W1qLXjoAmIA1WS8aoajLqOfckxHxoOtTEfjOP1IyZJUpMBIUlqMiBG7/ZRFzACjvnQcKiN+aAbr2sQkqQmZxCSpCYDQpLUZEDsA0mOTfL1JD/ofh6zi36/1vX5QZJfa+xfmeTR/iuevdmMOcnPJLkvyf9Ksj7J7+zb6mcuyflJnkiyIcnVjf1vTHJPt//hJGND+z7dtT+R5Lx9Wvgs7O2Yk5ybZHWSR7qf79nnxe+l2fydu/1vTfJCkk/us6LnQlX56vkF3ARc3b2/Grix0edYYGP385ju/TFD+38FuAt4dNTj6XvMwM8Af7vr8wbgW8AvjXpMjfrnAU8CJ3d1rgNOndbnt4DbuvcXAfd070/t+r8RWNqdZ96ox9TzmN8B/Gz3fhmwadTj6XvMQ/u/DPxn4JOjHs+evJxB7BsXAnd27+8E3t/ocx7w9ar6s6r6v8DXgfMBkhwJXAnc0H+pc2avx1xVL1bVfweoqpeBNcDi/kveY2cDG6pqY1fn3UyOe9jwf4cvA38nSbr2u6vqpar6IbChO9/+bq/HXFXfr6ofde3rgSOSvHGfVD07s/k7k+T9wA+ZHPMBxYDYN06oqme79z8GTmj0OQl4Zmh7omsDuB7418CLvVU492Y7ZgCSHA28D/hmDzXO1uvWP9ynql4FngeOm+Gx+6PZjHnYCmBNVb3UU51zaa/H3P3j7lPAZ/ZBnXPu8FEXcLBI8g3gxMaua4Y3qqqSzPja4iRnAH+lqv7F9M81R62vMQ+d/3Dg94HPV9XGvatS+5skpwE3Au8ddS37wLXALVX1QjehOKAYEHOkqv7urvYl+T9J3lJVzyZ5C/CnjW6bgF8c2l4MPAj8AjBI8hSTf683J3mwqn6REetxzFNuB35QVb87+2p7sQlYMrS9uGtr9ZnoAu9NwNYZHrs/ms2YSbIY+BpwSVU92X+5c2I2Y34n8MEkNwFHAzuSbKuqL/Re9VwY9SLIofACbua1C7Y3Nfocy+TnlMd0rx8Cx07rM8aBs0g9qzEzud7yFeCwUY9lN2M8nMmF9aX8dPHytGl9/hmvXby8t3t/Gq9dpN7IgbFIPZsxH931/5VRj2NfjXlan2s5wBapR17AofBi8vPXbwI/AL4x9D/BAfDvh/r9OpOLlRuAyxrnOZACYq/HzOS/0Ap4HFjbvT486jHtYpy/DPxvJq9yuaZruw64oHu/gMmrVzYA3wNOHjr2mu64J9gPr9Ka6zEDvw38v6G/6VrgzaMeT99/56FzHHAB4a02JElNXsUkSWoyICRJTQaEJKnJgJAkNRkQkqQmA0J6HUm2J1k79Nrpbp6zOPfYgXKHXh16/Ca19Pr+sqrOGHUR0r7mDELaS0meSnJT93yD7yX5+a59LMl/S/InSb6Z5K1d+wlJvpZkXff6G92p5iX5d92zL/4oyRFd/3+e5LHuPHePaJg6hBkQ0us7YtpHTB8a2vd8Vb0d+ALwu13bvwHurKq/Dvwe8Pmu/fPAH1fV6cCZ/PT2z6cAt1bVacBzTN7pFCZvUfKO7jwf62do0q75TWrpdSR5oaqObLQ/BbynqjYmmQ/8uKqOS7IFeEtVvdK1P1tVxyfZDCyuoVtcd3fo/XpVndJtfwqYX1U3JLkfeAH4A+APquqFnocqvYYzCGl2ahfv98TwMxG289O1wb8H3MrkbGNVd5dQaZ8xIKTZ+dDQz+907/8nk3f0BPhHTD4yFSZvXvibAEnmJXnTrk6a5DBgSU0+We9TTN4+eqdZjNQn/0Uivb4jkqwd2r6/qqYudT0myZ8wOQu4uGv7OPDFJFcBm4HLuvYrgNuT/AaTM4XfBJ6lbR7wn7oQCZMPTXpujsYjzYhrENJe6tYgBlW1ZdS1SH3wIyZJUpMzCElSkzMISVKTASFJajIgJElNBoQkqcmAkCQ1/X9g3LO1v8gOPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
