# 챗봇 만들기
1. sequence to sequence with attention
2. transformer (self-attention)

***

# 트랜스포머

### 기존 RNN 계열의 시퀀스투시퀀스 모델은 
- 각 스텝마다 단어가 입력되고, 은닉상태 벡터에 반영된다.
- 문장의 길이가 길던 짧던 벡터 한개에 모든 정보를 포함함
- 하나의 벡터에 표현하긴 부족할 것
- 각 시퀀스는 은닉상태로부터 누적되므로, 문장의 길이가 길면 초반에 나온건 손실이 많이 클 것이다.


### 트랜스 포머
- 문장이 긴 경우에 모든 단어의 정보를 잘 반영하도록 인코더와 디코더에 순환신경망을 사용하지 않고, __셀프어텐션__ 기법을 사용한다.
- 뿐만아니라 각 단어간의 유의미한 관계를 잡아낼 수 있다.
    
### 셀프 어텐션
- 각 단어끼리 얼마나 관계가 있는지를 계산해서 반영한다. (문장 하나에 대해 각 단어의 관계를 파악하는 것!)
- 문장 안에서 단어들 간의 관계를 측정할 수 있다.
- 이때 각 단어를 기준으로 다른 단어들과의 관계값을 계산함 (이 값을 __어텐션 스코어__라고 함)
    - 관계도가 큰 단어 간의 어텐션 점수는 높게 나올 것이다
    - 각 단어간의 관계를 측정한 값 == __어텐션 스코어__
    - 이 어텐션 스코어를 하나의 테이블로 만든 것 == __어텐션 맵__
- 이 어텐션 맵을 활용해 문장을 서로의 관계를 반영한 값으로 바꿔야 함
- 어텐션 스코어를 구하는 방식은 각 단어벡터의 유사도(=텍스트 유사도)를 구하는 것
    - 맨하탄, Dense 층을 거쳐 나온 값, 벡터끼리 내적
- 어텐션 맵에 소프트맥스 함수를 적용하면, 어텐션 맵이 특정 단어에 대한 다른 단어와의 연관도 값의 확률로 나타나짐
    - 스코어가 큰 경우 해당 단어와 관련이 높으므로 큰 값을 가져야함.
    - 이 확률 값과 기존 각 단어 벡터를 가중합(weighted sum) => 한 단어에 대한 context vector가 나옴.
- 이러한 방식으로 나머지 단어에 대해서도 동일하게 진행하면 셀프 어텐션이 끝난다.
    - 즉, 각 단어에 대한 contect vector가 나올 것!

### 셀프 어텐션 정리
1. 한 단어 A에 대해 다른 단어들과의 어텐션 스코어를 구함. 이 어텐션 스코어를 모은게 어텐션 맵.
2. 어텐션 맵에 softmax를 취해서, 각 어텐션 스코어를 확률값으로 나타냄.
3. 각 단어에 대한 어텐션 스코어와 각 단어의 단어벡터를 가중합 한다.
4. 결과적으로 단어 A에 대한 context vector 가 생성됨
5. 모든 단어에 대해 1~4를 진행한다.

***

## 트랜스포머 네트워크 모듈 목룍
1. 멀티헤드 어텐션 (Multi-head attention)
2. 서브시퀀트 마스크 어텐션 (Subsequent masked attention)
3. 포지션-와이드 피드 포워드 네트워크 (Position-wise feed fowrd network)
4. 리지듀얼 커넥션(Residual connection)


### 1. 멀티헤드 어텐션 (Multi-head attention)
- 셀프 어텐션 방식을 기반으로 구성됨
- 내적 어텐션 구조가 중첩된 형태이다.
#### 1) 스케일 내적 어텐션
- 입력은 3가지 이다. (query, key, value) 
- 특정단어(query) 에 대해 다른 단어(key, value)들과의 관계를 알아보는 것!
- 즉, query와 모든 key들의 어텐션 스코어를 구하고, 어텐션 스코어에 소프트맥스를 취해서 확률 값으로 만들고, 다시 value와 곱해서 가중합하면 context vector가 만들어진다.
- 스케일 내적 어텐션은 중간에 크기를 조정하는 과정(scaling)이 추가된 것!
    - 이유는 query, value를 이용해 내적한 값이 벡터의 차원이 커지면 학습이 잘 안 될 수 있으므로 벡터의 크기에 반비례하도록 크기를 조정하는 것임 (Attention is all you need 논문 참고) 
#### 2) 순방향 마스크 어텐션
- RNN의 경우 스텝이 존재해서 각 스텝마다 단어가 입력으로 들어가는 형태임
- 그에 반해 트랜스포머 모델은 __전체 문장이 한번에 행렬로 입력되는 구조임__ (스텝x)
- 이로 인해 생기는 문제점
    - 문장을 예측하는 과정에서 RNN은 자신보다 앞에있는 단어만 참고해서 단어를 예측
    - 하지만 트랜스포머는 전체 문장이 들어가기 때문에 위치와 상관없이 모든 단어를 참고해서 예측한다.
        - 이는 직관적으로 틀리다.
        - 자기 자신도 아직 예측을 하지 않았는데 뒤에 있는 단어를 예측?! 말도 안됨
- __자신보다 뒤에 있는 단어를 참고하지 않게 하는 기법이 마스크 기법이다!!!__

#### 3) 멀티 헤드 어텐션
- 셀프 어텐션에 대한 정보를 하나만 생성해서 모델에서 추론할 수도 있지만,
- 멀티 헤드 어텐션은 어텐션 맵을 여러개 만들어서 다양한 특징에 대한 어텐션을 볼 수 있게 한다.

### 2. 포지션-와이드 피드 포워드 네트워크 (Position-wise feed fowrd network)
- 셀프 어텐션 레이어를 거친 다음 피드 포워드 네트워크를 거침
- 한 문장에 있는 단어 토큰 벡터에 대해 각각 연산함

### 3. 리지듀얼 커넥션(Residual connection)
- x + F(x) 로 입력을 보존한다.
- 리지듀얼 커넥션 후에 층 정규화(layer normalization)을 수행
    - 층 정규화 : https://github.com/HYEZ/PyTorch-study/blob/master/ch5_%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D/10.%EA%B8%B0%EC%9A%B8%EA%B8%B0_%EC%86%8C%EC%8B%A4(Gradient%20Vanishing)%EA%B3%BC_%ED%8F%AD%EC%A3%BC(Exploding).ipynb
    - 기울기 소실 및 폭발을 막는 것 


### 4. 포지션 인코딩
- 트랜스포머 모델은 RNN과 다르게 순서대로가 아닌 한번에 입력을 넣음
- 따라서 순서를 반영해줄 필요가 있음
- 수식은 크게 2개로 나뉜다.
    - 1. 피처 차원(d_model)에서 인덱스가 짝수인 경우에 sin 함수를 할당한다.
    - 2. 피처 차원(d_model)에서 인덱스가 홀수인 경우 cos 함수 할당함